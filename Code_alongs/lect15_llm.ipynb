{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python-dotenv could not parse statement starting at line 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: c:\\Coding\\Python-Henrik-Pedersen-OPA2023\\Code_alongs\n",
      "Loading .env from: C:\\Coding\\Python-Henrik-Pedersen-OPA2023\\Code_alongs\\.env\n",
      "Loaded API Key: eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1c2VyX2lkIjoiMDBhZjM3NGYtMmYzMC00YTZjLWJjNDYtOTU3NGFlMTJjZDRkIiwidHlwZSI6ImFwaV90b2tlbiJ9.kbFX9COLB7UJPatWJGn4Vwgfr99IG515wNeSiy0HSOw\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Check the current working directory\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "\n",
    "# Load environment variables from .env file in the Code_alongs directory\n",
    "env_path = Path(\"C:\\Coding\\Python-Henrik-Pedersen-OPA2023\\Code_alongs\\.env\")\n",
    "print(\"Loading .env from:\", env_path)\n",
    "load_dotenv(env_path)\n",
    "\n",
    "# Get the API key from the environment variable\n",
    "API_Key = os.getenv(\"EDEN_API\")\n",
    "\n",
    "# Debug print to check if the key is loaded\n",
    "print(\"Loaded API Key:\", API_Key)\n",
    "\n",
    "# Check if the API key was successfully loaded\n",
    "if not API_Key:\n",
    "    raise ValueError(\"API key not found. Please check your .env file.\")\n",
    "\n",
    "# Set the headers with the loaded API key\n",
    "headers = {\"Authorization\": f\"Bearer {API_Key}\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversation History:\n",
      " \n",
      "\n",
      "Appended message: User: How are you?\n",
      "Updated Conversation History:\n",
      " \n",
      "User: How are you?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file_path = r\"C:\\Coding\\Python-Henrik-Pedersen-OPA2023\\Code_alongs\\conversation_history.txt\"\n",
    "\n",
    "\n",
    "def read_conversation_history(file_path):\n",
    "    try:\n",
    "        with open(file_path, \"r\") as file:\n",
    "            conv_history = file.read()\n",
    "        return conv_history\n",
    "    except FileNotFoundError:\n",
    "        return \"No conversation history found.\"\n",
    "\n",
    "\n",
    "def append_conversation_history(file_path, message):\n",
    "    with open(file_path, \"a\") as file:\n",
    "        file.write(message + \"\\n\")\n",
    "\n",
    "\n",
    "# Example usage\n",
    "# Read the conversation history\n",
    "conv_history = read_conversation_history(file_path)\n",
    "print(\"Conversation History:\\n\", conv_history)\n",
    "\n",
    "# Append a new message to the conversation history\n",
    "new_message = \"User: How are you?\"\n",
    "append_conversation_history(file_path, new_message)\n",
    "print(f\"Appended message: {new_message}\")\n",
    "\n",
    "# Verify the new message was appended\n",
    "conv_history = read_conversation_history(file_path)\n",
    "print(\"Updated Conversation History:\\n\", conv_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(prompt, provider=\"openai\", model=\"gpt-3.5-turbo-0125\", temperature=0.5):\n",
    "    headers = {\"Authorization\": f\"Bearer {API_Key}\"}\n",
    "\n",
    "    url = \"https://api.edenai.run/v2/text/chat\"\n",
    "    payload = {\n",
    "        \"providers\": provider,\n",
    "        \"model\": model,\n",
    "        \"text\": prompt,\n",
    "        \"chatbot_global_action\": \"Act as an assistant\",\n",
    "        \"previous_history\": [],\n",
    "        \"temperature\": temperature,\n",
    "        \"max_tokens\": 500,\n",
    "        \"fallback_providers\": \"perplexityai\",\n",
    "        \"show_original_response\": True\n",
    "    }\n",
    "\n",
    "    response = requests.post(url, json=payload, headers=headers)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Request failed with status code {response.status_code}: {response.text}\")\n",
    "\n",
    "    result = response.json()\n",
    "    original_response = result.get(\"openai\", {}).get(\"original_response\", {})\n",
    "    generated_text = result.get(\"openai\", {}).get(\"generated_text\", \"No response from provider.\")\n",
    "\n",
    "    # Print the token usage information\n",
    "    if 'usage' in original_response:\n",
    "        usage_info = original_response['usage']\n",
    "        print(f\"Prompt Tokens: {usage_info.get('prompt_tokens', 'N/A')}, \"\n",
    "                f\"Completion Tokens: {usage_info.get('completion_tokens', 'N/A')}, \"\n",
    "                f\"Total Tokens: {usage_info.get('total_tokens', 'N/A')}\")\n",
    "\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def startAI():\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            user_input = input(\"You: \")\n",
    "            if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "                break\n",
    "\n",
    "            # Append user input to the conversation history\n",
    "            append_conversation_history(file_path, f\"You: {user_input}\")\n",
    "\n",
    "            # Read the updated conversation history and prepare the prompt\n",
    "            updated_history = read_conversation_history(file_path)\n",
    "            prompt = \"\\n Perform the last task in context\"\n",
    "\n",
    "            print(f\"You: {user_input}\")\n",
    "            generated_text = chat(\n",
    "                f\"\"\"{prompt}: '''{updated_history}'''\"\"\")\n",
    "            \n",
    "            print(f\"You: {user_input}\")\n",
    "            print(f\"Bot: {generated_text}\")\n",
    "\n",
    "            # Append the AI response to the conversation history\n",
    "            append_conversation_history(file_path, f\"Bot: {generated_text}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You: Lets count up to 5 vertically\n",
      "Prompt Tokens: 38, Completion Tokens: 19, Total Tokens: 57\n",
      "Bot: Sure! Here is the continuation of the task:\n",
      "\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "You: lets continue counting for a random number between 1-15 but lets now have the count be horizontal\n",
      "Prompt Tokens: 83, Completion Tokens: 39, Total Tokens: 122\n",
      "Bot: Sure! Here is the continuation of the task:\n",
      "\n",
      "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15\n",
      "You: What was the random number?\n",
      "Prompt Tokens: 133, Completion Tokens: 8, Total Tokens: 141\n",
      "Bot: The random number chosen was 15.\n",
      "You: Could you explain to me how your prompt Token system work?\n",
      "Prompt Tokens: 157, Completion Tokens: 132, Total Tokens: 289\n",
      "Bot: I can explain how the prompt token system works:\n",
      "\n",
      "The prompt token system is a method used to break down a conversation into distinct parts or tasks. Each time a new task is initiated or requested, a prompt token is used to delineate the start of that task. This helps in keeping track of the different steps or actions within a conversation and allows for easier reference and continuation of tasks. \n",
      "\n",
      "In the context provided, the prompt tokens were used to separate the counting tasks into two parts: counting vertically and then counting horizontally for a random number. This system helps in organizing the conversation and ensuring that each task is completed and understood before moving on to the next one.\n"
     ]
    }
   ],
   "source": [
    "startAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'id': 'chatcmpl-9UrnXAX1NSXA1xS4FsjpxPgWrSSSt',\n",
       "  'object': 'chat.completion',\n",
       "  'created': 1717143667,\n",
       "  'model': 'gpt-3.5-turbo-0125',\n",
       "  'choices': [{'index': 0,\n",
       "    'message': {'role': 'assistant',\n",
       "     'content': \"It seems like you are asking how I am doing. Thank you for asking! I'm just a computer program, so I don't have feelings, but I'm here and ready to assist you. How can I help you today?\"},\n",
       "    'logprobs': None,\n",
       "    'finish_reason': 'stop'}],\n",
       "  'usage': {'prompt_tokens': 29, 'completion_tokens': 47, 'total_tokens': 76},\n",
       "  'system_fingerprint': None},\n",
       " \"It seems like you are asking how I am doing. Thank you for asking! I'm just a computer program, so I don't have feelings, but I'm here and ready to assist you. How can I help you today?\")"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_opa = chat(\n",
    "    f\"\"\"WHat does it say? '''{conv_history}'''\"\"\")\n",
    "\n",
    "summary_opa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
